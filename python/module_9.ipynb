{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: File Handling and I/O\n",
    "\n",
    "This module covers file operations, different file formats, streams, and input/output operations in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Basics\n",
    "\n",
    "### 1.1 Opening and Closing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory for our examples\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Working in temporary directory: {temp_dir}\")\n",
    "\n",
    "# Basic file opening\n",
    "file_path = os.path.join(temp_dir, \"example.txt\")\n",
    "\n",
    "# Writing to a file\n",
    "file = open(file_path, 'w')\n",
    "file.write(\"Hello, World!\\n\")\n",
    "file.write(\"This is a test file.\\n\")\n",
    "file.close()\n",
    "\n",
    "# Reading from a file\n",
    "file = open(file_path, 'r')\n",
    "content = file.read()\n",
    "print(f\"File content:\\n{content}\")\n",
    "file.close()\n",
    "\n",
    "# Using with statement (context manager) - recommended\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(f\"\\nUsing 'with' statement:\\n{content}\")\n",
    "# File is automatically closed after the with block\n",
    "\n",
    "# File modes\n",
    "modes = {\n",
    "    'r': 'Read (default)',\n",
    "    'w': 'Write (overwrites existing)',\n",
    "    'a': 'Append',\n",
    "    'x': 'Exclusive creation (fails if exists)',\n",
    "    'b': 'Binary mode',\n",
    "    't': 'Text mode (default)',\n",
    "    '+': 'Update (read and write)'\n",
    "}\n",
    "\n",
    "print(\"\\nFile modes:\")\n",
    "for mode, description in modes.items():\n",
    "    print(f\"  {mode}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample file with multiple lines\n",
    "sample_file = os.path.join(temp_dir, \"sample.txt\")\n",
    "with open(sample_file, 'w') as f:\n",
    "    f.write(\"Line 1\\n\")\n",
    "    f.write(\"Line 2\\n\")\n",
    "    f.write(\"Line 3\\n\")\n",
    "    f.write(\"Line 4\\n\")\n",
    "    f.write(\"Line 5\\n\")\n",
    "\n",
    "# Different ways to read files\n",
    "\n",
    "# 1. Read entire file\n",
    "with open(sample_file, 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"Read entire file:\")\n",
    "    print(content)\n",
    "\n",
    "# 2. Read specific number of characters\n",
    "with open(sample_file, 'r') as f:\n",
    "    chunk = f.read(10)\n",
    "    print(f\"First 10 characters: {repr(chunk)}\")\n",
    "\n",
    "# 3. Read line by line\n",
    "print(\"\\nReading line by line:\")\n",
    "with open(sample_file, 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        print(f\"  {repr(line)}\")\n",
    "        line = f.readline()\n",
    "\n",
    "# 4. Read all lines into a list\n",
    "with open(sample_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"\\nAll lines as list: {lines}\")\n",
    "\n",
    "# 5. Iterate over file object\n",
    "print(\"\\nIterating over file:\")\n",
    "with open(sample_file, 'r') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        print(f\"  Line {line_num}: {line.strip()}\")\n",
    "\n",
    "# 6. Read file in chunks\n",
    "def read_in_chunks(file_path, chunk_size=1024):\n",
    "    with open(file_path, 'r') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "print(\"\\nReading in chunks:\")\n",
    "for i, chunk in enumerate(read_in_chunks(sample_file, 10)):\n",
    "    print(f\"  Chunk {i}: {repr(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to write to files\n",
    "output_file = os.path.join(temp_dir, \"output.txt\")\n",
    "\n",
    "# 1. Write mode (overwrites)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"First line\\n\")\n",
    "    f.write(\"Second line\\n\")\n",
    "\n",
    "# 2. Append mode\n",
    "with open(output_file, 'a') as f:\n",
    "    f.write(\"Appended line\\n\")\n",
    "\n",
    "# 3. Write multiple lines\n",
    "lines = [\"Line A\\n\", \"Line B\\n\", \"Line C\\n\"]\n",
    "with open(output_file, 'a') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "# 4. Using print function\n",
    "with open(output_file, 'a') as f:\n",
    "    print(\"Printed line\", file=f)\n",
    "    print(\"Another printed line\", file=f)\n",
    "\n",
    "# Read and display the result\n",
    "with open(output_file, 'r') as f:\n",
    "    print(\"Final file content:\")\n",
    "    print(f.read())\n",
    "\n",
    "# 5. Writing with formatting\n",
    "data_file = os.path.join(temp_dir, \"data.txt\")\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"London\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Paris\"}\n",
    "]\n",
    "\n",
    "with open(data_file, 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"Name,Age,City\\n\")\n",
    "    # Write data\n",
    "    for person in data:\n",
    "        f.write(f\"{person['name']},{person['age']},{person['city']}\\n\")\n",
    "\n",
    "print(\"\\nFormatted data file:\")\n",
    "with open(data_file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Positioning and Seeking\n",
    "\n",
    "### 2.1 File Pointer Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file for seeking examples\n",
    "seek_file = os.path.join(temp_dir, \"seek_example.txt\")\n",
    "with open(seek_file, 'w') as f:\n",
    "    f.write(\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "# File seeking operations\n",
    "with open(seek_file, 'r') as f:\n",
    "    # tell() - get current position\n",
    "    print(f\"Initial position: {f.tell()}\")\n",
    "    \n",
    "    # Read 5 characters\n",
    "    data = f.read(5)\n",
    "    print(f\"Read: {data}\")\n",
    "    print(f\"Position after read: {f.tell()}\")\n",
    "    \n",
    "    # seek() - move to specific position\n",
    "    f.seek(10)\n",
    "    print(f\"\\nAfter seek(10): {f.tell()}\")\n",
    "    data = f.read(5)\n",
    "    print(f\"Read: {data}\")\n",
    "    \n",
    "    # Seek from current position (whence=1)\n",
    "    f.seek(5, 1)  # Move 5 positions forward from current\n",
    "    print(f\"\\nAfter seek(5, 1): {f.tell()}\")\n",
    "    data = f.read(5)\n",
    "    print(f\"Read: {data}\")\n",
    "    \n",
    "    # Seek from end (whence=2)\n",
    "    f.seek(-10, 2)  # Move 10 positions from end\n",
    "    print(f\"\\nAfter seek(-10, 2): {f.tell()}\")\n",
    "    data = f.read(5)\n",
    "    print(f\"Read: {data}\")\n",
    "    \n",
    "    # Reset to beginning\n",
    "    f.seek(0)\n",
    "    print(f\"\\nReset to beginning: {f.tell()}\")\n",
    "\n",
    "# Random access example\n",
    "def read_at_position(file_path, position, length):\n",
    "    with open(file_path, 'r') as f:\n",
    "        f.seek(position)\n",
    "        return f.read(length)\n",
    "\n",
    "print(f\"\\nRandom access:\")\n",
    "print(f\"Characters 10-15: {read_at_position(seek_file, 10, 5)}\")\n",
    "print(f\"Characters 20-25: {read_at_position(seek_file, 20, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Files\n",
    "\n",
    "### 3.1 Reading and Writing Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import pickle\n",
    "\n",
    "# Writing binary data\n",
    "binary_file = os.path.join(temp_dir, \"binary_data.bin\")\n",
    "\n",
    "# Write raw bytes\n",
    "with open(binary_file, 'wb') as f:\n",
    "    # Write bytes directly\n",
    "    f.write(b'Hello, Binary!')\n",
    "    f.write(bytes([65, 66, 67, 68]))  # ABCD\n",
    "    \n",
    "    # Write integers using struct\n",
    "    f.write(struct.pack('i', 42))  # 4-byte integer\n",
    "    f.write(struct.pack('f', 3.14159))  # 4-byte float\n",
    "\n",
    "# Read binary data\n",
    "with open(binary_file, 'rb') as f:\n",
    "    # Read first 14 bytes (Hello, Binary!)\n",
    "    text = f.read(14)\n",
    "    print(f\"Text: {text.decode()}\")\n",
    "    \n",
    "    # Read next 4 bytes\n",
    "    letters = f.read(4)\n",
    "    print(f\"Letters: {letters.decode()}\")\n",
    "    \n",
    "    # Read integer and float\n",
    "    int_bytes = f.read(4)\n",
    "    integer = struct.unpack('i', int_bytes)[0]\n",
    "    print(f\"Integer: {integer}\")\n",
    "    \n",
    "    float_bytes = f.read(4)\n",
    "    float_val = struct.unpack('f', float_bytes)[0]\n",
    "    print(f\"Float: {float_val}\")\n",
    "\n",
    "# Working with bytearray\n",
    "data = bytearray(b'Hello')\n",
    "print(f\"\\nBytearray: {data}\")\n",
    "data[0] = ord('J')  # Change H to J\n",
    "print(f\"Modified: {data}\")\n",
    "print(f\"As string: {data.decode()}\")\n",
    "\n",
    "# Binary file copy\n",
    "def copy_binary_file(source, destination):\n",
    "    with open(source, 'rb') as src:\n",
    "        with open(destination, 'wb') as dst:\n",
    "            while True:\n",
    "                chunk = src.read(1024)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                dst.write(chunk)\n",
    "\n",
    "copy_dest = os.path.join(temp_dir, \"binary_copy.bin\")\n",
    "copy_binary_file(binary_file, copy_dest)\n",
    "print(f\"\\nBinary file copied to {copy_dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pickle - Python Object Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Pickling Python objects\n",
    "pickle_file = os.path.join(temp_dir, \"data.pickle\")\n",
    "\n",
    "# Data to pickle\n",
    "data = {\n",
    "    'name': 'Alice',\n",
    "    'age': 30,\n",
    "    'hobbies': ['reading', 'swimming', 'coding'],\n",
    "    'scores': (95, 87, 92),\n",
    "    'address': {\n",
    "        'street': '123 Main St',\n",
    "        'city': 'New York'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "print(\"Data pickled successfully\")\n",
    "\n",
    "# Load from pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "print(f\"\\nLoaded data: {loaded_data}\")\n",
    "print(f\"Data matches original: {data == loaded_data}\")\n",
    "\n",
    "# Pickling custom objects\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.id = id(self)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Person(name='{self.name}', age={self.age})\"\n",
    "\n",
    "# Create and pickle custom objects\n",
    "people = [\n",
    "    Person(\"Alice\", 30),\n",
    "    Person(\"Bob\", 25),\n",
    "    Person(\"Charlie\", 35)\n",
    "]\n",
    "\n",
    "people_file = os.path.join(temp_dir, \"people.pickle\")\n",
    "with open(people_file, 'wb') as f:\n",
    "    pickle.dump(people, f)\n",
    "\n",
    "with open(people_file, 'rb') as f:\n",
    "    loaded_people = pickle.load(f)\n",
    "\n",
    "print(\"\\nOriginal people:\")\n",
    "for p in people:\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print(\"\\nLoaded people:\")\n",
    "for p in loaded_people:\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "# Pickle to string\n",
    "pickled_string = pickle.dumps(data)\n",
    "print(f\"\\nPickled to bytes: {pickled_string[:50]}...\")\n",
    "unpickled = pickle.loads(pickled_string)\n",
    "print(f\"Unpickled from bytes: {unpickled['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Different File Formats\n",
    "\n",
    "### 4.1 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Writing CSV files\n",
    "csv_file = os.path.join(temp_dir, \"data.csv\")\n",
    "\n",
    "# Write with csv.writer\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Name', 'Age', 'City'])  # Header\n",
    "    writer.writerow(['Alice', 30, 'New York'])\n",
    "    writer.writerow(['Bob', 25, 'London'])\n",
    "    writer.writerows([  # Multiple rows at once\n",
    "        ['Charlie', 35, 'Paris'],\n",
    "        ['Diana', 28, 'Tokyo']\n",
    "    ])\n",
    "\n",
    "# Read CSV file\n",
    "print(\"Reading CSV with csv.reader:\")\n",
    "with open(csv_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "# Using DictWriter and DictReader\n",
    "dict_csv_file = os.path.join(temp_dir, \"dict_data.csv\")\n",
    "\n",
    "# Write with DictWriter\n",
    "fieldnames = ['id', 'name', 'email', 'department']\n",
    "employees = [\n",
    "    {'id': 1, 'name': 'Alice', 'email': 'alice@example.com', 'department': 'IT'},\n",
    "    {'id': 2, 'name': 'Bob', 'email': 'bob@example.com', 'department': 'HR'},\n",
    "    {'id': 3, 'name': 'Charlie', 'email': 'charlie@example.com', 'department': 'Sales'}\n",
    "]\n",
    "\n",
    "with open(dict_csv_file, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(employees)\n",
    "\n",
    "# Read with DictReader\n",
    "print(\"\\nReading CSV with DictReader:\")\n",
    "with open(dict_csv_file, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        print(f\"  {row['name']}: {row['email']} ({row['department']})\")\n",
    "\n",
    "# Custom CSV dialect\n",
    "custom_csv_file = os.path.join(temp_dir, \"custom.csv\")\n",
    "\n",
    "csv.register_dialect('pipes', delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "with open(custom_csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='pipes')\n",
    "    writer.writerow(['Name', 'Description', 'Price'])\n",
    "    writer.writerow(['Product A', 'Description with, comma', 19.99])\n",
    "    writer.writerow(['Product B', 'Another item', 29.99])\n",
    "\n",
    "print(\"\\nCustom dialect CSV:\")\n",
    "with open(custom_csv_file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON data\n",
    "data = {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"active\": True,\n",
    "    \"balance\": 1234.56,\n",
    "    \"tags\": [\"python\", \"developer\", \"json\"],\n",
    "    \"address\": {\n",
    "        \"street\": \"123 Main St\",\n",
    "        \"city\": \"New York\",\n",
    "        \"zip\": \"10001\"\n",
    "    },\n",
    "    \"phone\": None\n",
    "}\n",
    "\n",
    "# Write JSON to file\n",
    "json_file = os.path.join(temp_dir, \"data.json\")\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"JSON file content:\")\n",
    "with open(json_file, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Read JSON from file\n",
    "with open(json_file, 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "    print(f\"\\nLoaded data type: {type(loaded_data)}\")\n",
    "    print(f\"Name: {loaded_data['name']}\")\n",
    "    print(f\"City: {loaded_data['address']['city']}\")\n",
    "\n",
    "# JSON with custom encoder\n",
    "from datetime import datetime, date\n",
    "\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime, date)):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "data_with_date = {\n",
    "    \"event\": \"Meeting\",\n",
    "    \"date\": datetime.now(),\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}\n",
    "\n",
    "json_date_file = os.path.join(temp_dir, \"event.json\")\n",
    "with open(json_date_file, 'w') as f:\n",
    "    json.dump(data_with_date, f, cls=DateTimeEncoder, indent=2)\n",
    "\n",
    "print(\"\\nJSON with datetime:\")\n",
    "with open(json_date_file, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Pretty printing JSON\n",
    "complex_data = {\n",
    "    \"users\": [\n",
    "        {\"id\": 1, \"name\": \"Alice\", \"roles\": [\"admin\", \"user\"]},\n",
    "        {\"id\": 2, \"name\": \"Bob\", \"roles\": [\"user\"]}\n",
    "    ],\n",
    "    \"settings\": {\n",
    "        \"theme\": \"dark\",\n",
    "        \"notifications\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nPretty printed JSON:\")\n",
    "print(json.dumps(complex_data, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "# Creating XML\n",
    "root = ET.Element(\"catalog\")\n",
    "\n",
    "# Add books\n",
    "book1 = ET.SubElement(root, \"book\", id=\"1\")\n",
    "ET.SubElement(book1, \"title\").text = \"Python Programming\"\n",
    "ET.SubElement(book1, \"author\").text = \"John Doe\"\n",
    "ET.SubElement(book1, \"year\").text = \"2023\"\n",
    "ET.SubElement(book1, \"price\").text = \"29.99\"\n",
    "\n",
    "book2 = ET.SubElement(root, \"book\", id=\"2\")\n",
    "ET.SubElement(book2, \"title\").text = \"Data Science\"\n",
    "ET.SubElement(book2, \"author\").text = \"Jane Smith\"\n",
    "ET.SubElement(book2, \"year\").text = \"2024\"\n",
    "ET.SubElement(book2, \"price\").text = \"39.99\"\n",
    "\n",
    "# Create tree and save to file\n",
    "tree = ET.ElementTree(root)\n",
    "xml_file = os.path.join(temp_dir, \"catalog.xml\")\n",
    "tree.write(xml_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "# Pretty print XML\n",
    "def prettify_xml(elem):\n",
    "    rough_string = ET.tostring(elem, encoding='unicode')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "\n",
    "print(\"XML content:\")\n",
    "print(prettify_xml(root))\n",
    "\n",
    "# Parse XML file\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"Parsed XML data:\")\n",
    "for book in root.findall('book'):\n",
    "    book_id = book.get('id')\n",
    "    title = book.find('title').text\n",
    "    author = book.find('author').text\n",
    "    price = book.find('price').text\n",
    "    print(f\"  Book {book_id}: {title} by {author} (${price})\")\n",
    "\n",
    "# Modify XML\n",
    "for book in root.findall('book'):\n",
    "    price_elem = book.find('price')\n",
    "    current_price = float(price_elem.text)\n",
    "    # Apply 10% discount\n",
    "    new_price = current_price * 0.9\n",
    "    price_elem.text = f\"{new_price:.2f}\"\n",
    "    # Add discount attribute\n",
    "    price_elem.set('discounted', 'true')\n",
    "\n",
    "# Save modified XML\n",
    "tree.write(xml_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(\"\\nModified XML with discounts:\")\n",
    "tree = ET.parse(xml_file)\n",
    "print(prettify_xml(tree.getroot()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. File and Directory Operations\n",
    "\n",
    "### 5.1 Working with Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Using os.path\n",
    "print(\"Using os.path:\")\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Path operations\n",
    "file_path = os.path.join(temp_dir, \"test\", \"file.txt\")\n",
    "print(f\"\\nJoined path: {file_path}\")\n",
    "print(f\"Directory: {os.path.dirname(file_path)}\")\n",
    "print(f\"Filename: {os.path.basename(file_path)}\")\n",
    "print(f\"Split: {os.path.split(file_path)}\")\n",
    "print(f\"Extension: {os.path.splitext(file_path)}\")\n",
    "\n",
    "# Check path properties\n",
    "test_path = temp_dir\n",
    "print(f\"\\nPath properties for {test_path}:\")\n",
    "print(f\"Exists: {os.path.exists(test_path)}\")\n",
    "print(f\"Is file: {os.path.isfile(test_path)}\")\n",
    "print(f\"Is directory: {os.path.isdir(test_path)}\")\n",
    "print(f\"Absolute: {os.path.isabs(test_path)}\")\n",
    "\n",
    "# Using pathlib (modern approach)\n",
    "print(\"\\nUsing pathlib:\")\n",
    "path = Path(temp_dir)\n",
    "print(f\"Path: {path}\")\n",
    "print(f\"Name: {path.name}\")\n",
    "print(f\"Parent: {path.parent}\")\n",
    "print(f\"Suffix: {path.suffix}\")\n",
    "print(f\"Parts: {path.parts}\")\n",
    "\n",
    "# Create new path\n",
    "new_file = path / \"subfolder\" / \"newfile.txt\"\n",
    "print(f\"\\nNew path: {new_file}\")\n",
    "print(f\"Parent dirs: {list(new_file.parents)}\")\n",
    "\n",
    "# Path methods\n",
    "test_file = Path(temp_dir) / \"test.txt\"\n",
    "test_file.write_text(\"Hello from pathlib!\")\n",
    "print(f\"\\nFile content: {test_file.read_text()}\")\n",
    "print(f\"File size: {test_file.stat().st_size} bytes\")\n",
    "\n",
    "# Iterating directory\n",
    "print(f\"\\nFiles in {temp_dir}:\")\n",
    "for item in Path(temp_dir).iterdir():\n",
    "    if item.is_file():\n",
    "        print(f\"  File: {item.name} ({item.stat().st_size} bytes)\")\n",
    "    elif item.is_dir():\n",
    "        print(f\"  Dir: {item.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Directory Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Create directories\n",
    "test_dir = os.path.join(temp_dir, \"test_directory\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "nested_dir = os.path.join(temp_dir, \"parent\", \"child\", \"grandchild\")\n",
    "os.makedirs(nested_dir, exist_ok=True)\n",
    "print(f\"Created nested directory: {nested_dir}\")\n",
    "\n",
    "# List directory contents\n",
    "print(f\"\\nContents of {temp_dir}:\")\n",
    "for item in os.listdir(temp_dir):\n",
    "    item_path = os.path.join(temp_dir, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"  [DIR] {item}\")\n",
    "    else:\n",
    "        print(f\"  [FILE] {item}\")\n",
    "\n",
    "# Walk directory tree\n",
    "print(\"\\nWalking directory tree:\")\n",
    "for root, dirs, files in os.walk(temp_dir):\n",
    "    level = root.replace(temp_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:3]:  # Limit output\n",
    "        print(f\"{subindent}{file}\")\n",
    "\n",
    "# Copy files and directories\n",
    "source_file = os.path.join(temp_dir, \"test.txt\")\n",
    "dest_file = os.path.join(test_dir, \"test_copy.txt\")\n",
    "shutil.copy2(source_file, dest_file)  # Preserves metadata\n",
    "print(f\"\\nCopied file to {dest_file}\")\n",
    "\n",
    "# Copy entire directory\n",
    "source_dir = test_dir\n",
    "backup_dir = os.path.join(temp_dir, \"backup\")\n",
    "shutil.copytree(source_dir, backup_dir, dirs_exist_ok=True)\n",
    "print(f\"Backed up directory to {backup_dir}\")\n",
    "\n",
    "# Move/rename\n",
    "old_path = os.path.join(temp_dir, \"test.txt\")\n",
    "new_path = os.path.join(temp_dir, \"renamed.txt\")\n",
    "if os.path.exists(old_path):\n",
    "    shutil.move(old_path, new_path)\n",
    "    print(f\"Renamed {old_path} to {new_path}\")\n",
    "\n",
    "# Using glob for pattern matching\n",
    "print(\"\\nFiles matching pattern '*.txt':\")\n",
    "for file in glob.glob(os.path.join(temp_dir, \"*.txt\")):\n",
    "    print(f\"  {os.path.basename(file)}\")\n",
    "\n",
    "print(\"\\nFiles matching pattern '**/*.csv' (recursive):\")\n",
    "for file in glob.glob(os.path.join(temp_dir, \"**\", \"*.csv\"), recursive=True):\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporary Files and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Create temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.tmp') as tmp:\n",
    "    tmp.write(\"Temporary data\")\n",
    "    tmp_name = tmp.name\n",
    "    print(f\"Temporary file: {tmp_name}\")\n",
    "\n",
    "# Read temporary file\n",
    "with open(tmp_name, 'r') as f:\n",
    "    print(f\"Content: {f.read()}\")\n",
    "\n",
    "# Clean up\n",
    "os.unlink(tmp_name)\n",
    "\n",
    "# Temporary file with context manager (auto-delete)\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt') as tmp:\n",
    "    tmp.write(\"This will be deleted automatically\")\n",
    "    tmp.flush()\n",
    "    print(f\"Temp file (auto-delete): {tmp.name}\")\n",
    "    # File is automatically deleted when leaving context\n",
    "\n",
    "# Temporary directory\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    print(f\"\\nTemporary directory: {tmpdir}\")\n",
    "    \n",
    "    # Create files in temp directory\n",
    "    temp_file = os.path.join(tmpdir, \"temp.txt\")\n",
    "    with open(temp_file, 'w') as f:\n",
    "        f.write(\"Temporary file in temporary directory\")\n",
    "    \n",
    "    print(f\"Files in temp dir: {os.listdir(tmpdir)}\")\n",
    "    # Directory and contents are deleted when leaving context\n",
    "\n",
    "# Get temp directory path\n",
    "print(f\"\\nSystem temp directory: {tempfile.gettempdir()}\")\n",
    "\n",
    "# Create temporary file with specific prefix\n",
    "with tempfile.NamedTemporaryFile(\n",
    "    prefix='myapp_',\n",
    "    suffix='.dat',\n",
    "    dir=temp_dir\n",
    ") as tmp:\n",
    "    print(f\"Custom temp file: {tmp.name}\")\n",
    "\n",
    "# SpooledTemporaryFile (memory then disk)\n",
    "with tempfile.SpooledTemporaryFile(max_size=1024) as tmp:\n",
    "    tmp.write(b\"Small data in memory\")\n",
    "    print(f\"In memory: {tmp._rolled}\")\n",
    "    \n",
    "    # Write more data to exceed max_size\n",
    "    tmp.write(b\"x\" * 2000)\n",
    "    print(f\"Rolled to disk: {tmp._rolled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. File Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "import tarfile\n",
    "\n",
    "# Working with ZIP files\n",
    "zip_path = os.path.join(temp_dir, \"archive.zip\")\n",
    "\n",
    "# Create ZIP archive\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add files\n",
    "    for file in glob.glob(os.path.join(temp_dir, \"*.txt\")):\n",
    "        arcname = os.path.basename(file)\n",
    "        zipf.write(file, arcname)\n",
    "        print(f\"Added {arcname} to ZIP\")\n",
    "\n",
    "# List ZIP contents\n",
    "print(\"\\nZIP archive contents:\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "    for info in zipf.infolist():\n",
    "        print(f\"  {info.filename}: {info.file_size} bytes -> {info.compress_size} compressed\")\n",
    "\n",
    "# Extract from ZIP\n",
    "extract_dir = os.path.join(temp_dir, \"extracted\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "    zipf.extractall(extract_dir)\n",
    "    print(f\"\\nExtracted to {extract_dir}\")\n",
    "\n",
    "# Working with GZIP\n",
    "text_file = os.path.join(temp_dir, \"large_text.txt\")\n",
    "with open(text_file, 'w') as f:\n",
    "    f.write(\"This is a line of text.\\n\" * 1000)\n",
    "\n",
    "gz_file = os.path.join(temp_dir, \"compressed.gz\")\n",
    "\n",
    "# Compress with gzip\n",
    "with open(text_file, 'rb') as f_in:\n",
    "    with gzip.open(gz_file, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "original_size = os.path.getsize(text_file)\n",
    "compressed_size = os.path.getsize(gz_file)\n",
    "print(f\"\\nGZIP compression:\")\n",
    "print(f\"Original: {original_size} bytes\")\n",
    "print(f\"Compressed: {compressed_size} bytes\")\n",
    "print(f\"Compression ratio: {original_size/compressed_size:.2f}x\")\n",
    "\n",
    "# Read from gzip\n",
    "with gzip.open(gz_file, 'rt') as f:\n",
    "    first_line = f.readline()\n",
    "    print(f\"\\nFirst line from gzip: {first_line.strip()}\")\n",
    "\n",
    "# Working with TAR archives\n",
    "tar_path = os.path.join(temp_dir, \"archive.tar.gz\")\n",
    "\n",
    "# Create TAR archive\n",
    "with tarfile.open(tar_path, 'w:gz') as tar:\n",
    "    for file in glob.glob(os.path.join(temp_dir, \"*.csv\")):\n",
    "        tar.add(file, arcname=os.path.basename(file))\n",
    "        print(f\"Added {os.path.basename(file)} to TAR\")\n",
    "\n",
    "# List TAR contents\n",
    "print(\"\\nTAR archive contents:\")\n",
    "with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        print(f\"  {member.name}: {member.size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced I/O Operations\n",
    "\n",
    "### 8.1 String I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "\n",
    "# StringIO - in-memory text stream\n",
    "string_buffer = StringIO()\n",
    "string_buffer.write(\"Hello, \")\n",
    "string_buffer.write(\"StringIO!\\n\")\n",
    "string_buffer.write(\"Line 2\\n\")\n",
    "\n",
    "# Get value\n",
    "content = string_buffer.getvalue()\n",
    "print(f\"StringIO content:\\n{content}\")\n",
    "\n",
    "# Reset and read\n",
    "string_buffer.seek(0)\n",
    "print(f\"Read line: {string_buffer.readline()}\")\n",
    "\n",
    "# Use StringIO as file\n",
    "def process_file(file_obj):\n",
    "    \"\"\"Function that expects a file-like object\"\"\"\n",
    "    lines = file_obj.readlines()\n",
    "    return len(lines)\n",
    "\n",
    "string_buffer.seek(0)\n",
    "line_count = process_file(string_buffer)\n",
    "print(f\"Line count: {line_count}\")\n",
    "\n",
    "# BytesIO - in-memory bytes stream\n",
    "bytes_buffer = BytesIO()\n",
    "bytes_buffer.write(b\"Binary data: \")\n",
    "bytes_buffer.write(bytes([65, 66, 67, 68]))  # ABCD\n",
    "\n",
    "bytes_content = bytes_buffer.getvalue()\n",
    "print(f\"\\nBytesIO content: {bytes_content}\")\n",
    "print(f\"Decoded: {bytes_content.decode()}\")\n",
    "\n",
    "# CSV with StringIO\n",
    "csv_buffer = StringIO()\n",
    "writer = csv.writer(csv_buffer)\n",
    "writer.writerow(['Name', 'Score'])\n",
    "writer.writerow(['Alice', 95])\n",
    "writer.writerow(['Bob', 87])\n",
    "\n",
    "csv_data = csv_buffer.getvalue()\n",
    "print(f\"\\nCSV in memory:\\n{csv_data}\")\n",
    "\n",
    "# Read CSV from StringIO\n",
    "csv_buffer.seek(0)\n",
    "reader = csv.DictReader(csv_buffer)\n",
    "for row in reader:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Memory-Mapped Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "\n",
    "# Create a file for memory mapping\n",
    "mmap_file = os.path.join(temp_dir, \"mmap_test.dat\")\n",
    "size = 1024  # 1KB\n",
    "\n",
    "# Create file with initial data\n",
    "with open(mmap_file, 'wb') as f:\n",
    "    f.write(b'\\x00' * size)  # Fill with zeros\n",
    "\n",
    "# Memory-map the file\n",
    "with open(mmap_file, 'r+b') as f:\n",
    "    with mmap.mmap(f.fileno(), 0) as mmapped:\n",
    "        # Write to memory-mapped file\n",
    "        mmapped[0:5] = b'Hello'\n",
    "        mmapped[10:15] = b'World'\n",
    "        \n",
    "        # Read from memory-mapped file\n",
    "        print(f\"Data at 0-5: {mmapped[0:5]}\")\n",
    "        print(f\"Data at 10-15: {mmapped[10:15]}\")\n",
    "        \n",
    "        # Find in memory-mapped file\n",
    "        index = mmapped.find(b'World')\n",
    "        print(f\"Found 'World' at index: {index}\")\n",
    "        \n",
    "        # Slice operations\n",
    "        mmapped[100:110] = b'0123456789'\n",
    "        print(f\"Data at 100-110: {mmapped[100:110]}\")\n",
    "\n",
    "# Verify changes were written\n",
    "with open(mmap_file, 'rb') as f:\n",
    "    f.seek(0)\n",
    "    print(f\"\\nFirst 20 bytes: {f.read(20)}\")\n",
    "\n",
    "print(f\"\\nMemory-mapped file operations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. File Locking and Concurrent Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fcntl  # Unix/Linux only\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Platform-independent file locking example\n",
    "class FileLock:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.lock_file = filename + '.lock'\n",
    "    \n",
    "    def acquire(self, timeout=10):\n",
    "        start_time = time.time()\n",
    "        while os.path.exists(self.lock_file):\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(f\"Could not acquire lock for {self.filename}\")\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # Create lock file\n",
    "        with open(self.lock_file, 'w') as f:\n",
    "            f.write(str(os.getpid()))\n",
    "    \n",
    "    def release(self):\n",
    "        if os.path.exists(self.lock_file):\n",
    "            os.remove(self.lock_file)\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.acquire()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.release()\n",
    "\n",
    "# Using file lock\n",
    "shared_file = os.path.join(temp_dir, \"shared.txt\")\n",
    "\n",
    "def write_with_lock(thread_id, message):\n",
    "    with FileLock(shared_file):\n",
    "        print(f\"Thread {thread_id} acquired lock\")\n",
    "        with open(shared_file, 'a') as f:\n",
    "            f.write(f\"Thread {thread_id}: {message}\\n\")\n",
    "            time.sleep(0.5)  # Simulate work\n",
    "        print(f\"Thread {thread_id} released lock\")\n",
    "\n",
    "# Create threads\n",
    "threads = []\n",
    "for i in range(3):\n",
    "    t = threading.Thread(target=write_with_lock, args=(i, f\"Message {i}\"))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# Read result\n",
    "print(\"\\nShared file content:\")\n",
    "with open(shared_file, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Atomic file operations\n",
    "def atomic_write(filename, data):\n",
    "    \"\"\"Write to file atomically using temp file and rename\"\"\"\n",
    "    temp_name = filename + '.tmp'\n",
    "    \n",
    "    # Write to temp file\n",
    "    with open(temp_name, 'w') as f:\n",
    "        f.write(data)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())  # Force write to disk\n",
    "    \n",
    "    # Atomic rename\n",
    "    os.replace(temp_name, filename)\n",
    "\n",
    "atomic_file = os.path.join(temp_dir, \"atomic.txt\")\n",
    "atomic_write(atomic_file, \"This was written atomically\")\n",
    "print(f\"\\nAtomic write completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Cleaned up temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Summary\n",
    "\n",
    "This module covered comprehensive file handling and I/O operations in Python:\n",
    "\n",
    "1. **File Basics**: Opening, reading, writing, file modes\n",
    "2. **File Positioning**: seek(), tell(), random access\n",
    "3. **Binary Files**: Binary I/O, struct module, pickle serialization\n",
    "4. **File Formats**: CSV, JSON, XML handling\n",
    "5. **Path Operations**: os.path and pathlib for path manipulation\n",
    "6. **Directory Operations**: Creating, listing, walking directories\n",
    "7. **Temporary Files**: Safe temporary file and directory creation\n",
    "8. **Compression**: ZIP, GZIP, TAR archives\n",
    "9. **Advanced I/O**: StringIO, BytesIO, memory-mapped files\n",
    "10. **Concurrent Access**: File locking, atomic operations\n",
    "\n",
    "Key takeaways:\n",
    "- Always use context managers (with statement) for file operations\n",
    "- Choose appropriate file mode for your use case\n",
    "- Use pathlib for modern path manipulation\n",
    "- Consider memory-mapped files for large file operations\n",
    "- Implement proper locking for concurrent file access\n",
    "- Use appropriate serialization format (JSON, pickle, CSV) for your data\n",
    "- Handle encoding properly when working with text files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}